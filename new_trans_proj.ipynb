{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsuna_9sx8OJ",
        "outputId": "9c712dd3-8312-4e38-e11f-8b2438fccd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=c327f4c1b6eca92e80f1ae64d7378dfd3f15235df921661314017ffedb8bac7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.4.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=83b66a3a0569be30e7c8f0716795b6ae1f26446e075c06f4f89f8309bbda03bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install spacy\n",
        "!pip install langdetect\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poem = \"\"\"Twinkle twinkle little star, how i wonder what you are, up above the world so high, like a diamond in the sky.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9URzvKb0MJ95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poem=poem.lower()"
      ],
      "metadata": {
        "id": "P9dvFzD0dPaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "from langdetect import detect\n",
        "\n",
        "def translate_poem(poem):\n",
        "    translator = Translator()\n",
        "    source_language = detect(poem)\n",
        "    translation = translator.translate(poem, src=source_language, dest='en')\n",
        "    return translation.text\n",
        "\n",
        "\n",
        "# Translate the poem to English\n",
        "translated_poem = translate_poem(poem)\n",
        "print(\"Translated Poem:\")\n",
        "print(translated_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUlWlpP0x_hG",
        "outputId": "af1138ff-6b8a-46f8-ccd7-659e0264cd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Poem:\n",
            "twinkle twinkle little star, how i wonder what you are, up above the world so high, like a diamond in the sky.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def simplify_poem(poem):\n",
        "    # Tokenization\n",
        "    words = nltk.word_tokenize(poem)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Joining the lemmatized words back into poem\n",
        "    simplified_poem = ' '.join(lemmatized_words)\n",
        "\n",
        "\n",
        "    return simplified_poem\n",
        "\n",
        "\n",
        "simplified_poem=simplify_poem(translated_poem)\n",
        "\n",
        "print(simplified_poem)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fQvFL3WbXPM",
        "outputId": "5bfca786-c96f-4b2c-e4cb-d2993391eaed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twinkle twinkle little star , how i wonder what you are , up above the world so high , like a diamond in the sky .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing apostrophes\n",
        "cleaned_poem = simplified_poem.replace(\"'\", \"\").replace(\"s'\", \" \")\n",
        "\n",
        "print(\"Original Poem:\")\n",
        "print(poem)\n",
        "\n",
        "print(\"\\nPoem after removing apostrophe and 's':\")\n",
        "print(cleaned_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y9j37xNUVNn",
        "outputId": "29d738dd-4367-4994-ceb2-c2fc7257d097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Poem:\n",
            "twinkle twinkle little star, how i wonder what you are, up above the world so high, like a diamond in the sky.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Poem after removing apostrophe and 's':\n",
            "twinkle twinkle little star , how i wonder what you are , up above the world so high , like a diamond in the sky .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "    # Define the set of punctuation characters\n",
        "    punctuations = set(string.punctuation)\n",
        "    # Remove punctuations from the sentence\n",
        "    sentence_without_punctuation = ''.join(char for char in sentence if char not in punctuations)\n",
        "    return sentence_without_punctuation\n",
        "\n",
        "\n",
        "sentence_without_punctuation = remove_punctuation(cleaned_poem)\n",
        "print(\"Sentence without punctuations:\", sentence_without_punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eigyPyzGmIAD",
        "outputId": "802c585e-286c-4352-eba6-f036424e5dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence without punctuations: twinkle twinkle little star  how i wonder what you are  up above the world so high  like a diamond in the sky \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def resimplify_poem(poem):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(poem)\n",
        "\n",
        "    # filter out unnecessary words\n",
        "    resimplified_poem = \" \".join([token.text for token in doc if token.pos_ in ['NOUN']])\n",
        "\n",
        "    return resimplified_poem\n",
        "\n",
        "input_poem = simplified_poem\n",
        "\n",
        "resimplified_result = resimplify_poem(sentence_without_punctuation)\n",
        "print(\"Original Poem:\")\n",
        "print(cleaned_poem)\n",
        "print(resimplified_result)\n"
      ],
      "metadata": {
        "id": "HepgvFvEOdzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab040e7-8b57-407a-9bbe-5869d8289822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Poem:\n",
            "twinkle twinkle little star , how i wonder what you are , up above the world so high , like a diamond in the sky .\n",
            "twinkle twinkle star world diamond sky\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shallow filtering\n",
        "def remove_duplicate_words(sentence):\n",
        "    words = sentence.split()\n",
        "    unique_words = set()\n",
        "    for word in words:\n",
        "        unique_words.add(word)\n",
        "    unique_sentence = \" \".join(unique_words)\n",
        "    return unique_sentence\n",
        "\n",
        "\n",
        "unique_sentence = remove_duplicate_words(resimplified_result)\n",
        "print(\"after shallow filtering:\", unique_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOBCLlGTk611",
        "outputId": "24b2c489-aa16-4dac-da9a-3bbbc1c5d5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence with duplicate words removed: star world sky twinkle diamond\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deep filtering\n",
        "def filter_words(sentence, dataset):\n",
        "    # Split the sentence into individual words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Initialize an empty list to store words from the sentence that are in the dataset\n",
        "    filtered_words = []\n",
        "\n",
        "    # Iterate through each word in the sentence\n",
        "    for word in words:\n",
        "        # Check if the word is in the dataset\n",
        "        if word.lower() in dataset:\n",
        "            # If the word is in the dataset, add it to the filtered words list\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    # Join the filtered words list into a single string separated by space\n",
        "    filtered_sentence = ' '.join(filtered_words)\n",
        "\n",
        "    return filtered_sentence\n",
        "\n",
        "# dataset of nature-related words\n",
        "nature_dataset = [\n",
        "    \"tree\", \"bird\", \"sun\", \"lake\", \"river\",\n",
        "    \"flower\", \"mountain\", \"cloud\", \"sky\", \"ocean\",\n",
        "    \"forest\", \"meadow\", \"rain\", \"snow\", \"beach\", \"butterfly\",\n",
        "    \"bee\", \"grass\", \"leaf\", \"rock\", \"valley\",\n",
        "    \"stream\", \"pond\", \"marsh\", \"desert\", \"canyon\",\n",
        "    \"hill\", \"sunset\", \"sunrise\", \"rainbow\", \"waterfall\",\n",
        "    \"thunder\", \"lightning\", \"wind\", \"dew\", \"frost\",\n",
        "    \"ice\", \"sand\", \"pebble\", \"moss\", \"fern\",\n",
        "    \"pine\", \"oak\", \"maple\", \"birch\", \"willow\",\n",
        "    \"palm\", \"bamboo\", \"cedar\", \"redwood\", \"nature\", \"night\", \"day\", \"morning\", \"cypress\",\n",
        "    \"sequoia\", \"eagle\", \"hawk\", \"sparrow\", \"finch\",\n",
        "    \"robin\", \"owl\", \"heron\", \"crane\", \"swan\",\n",
        "    \"duck\", \"goose\", \"pelican\", \"star\", \"seagull\", \"albatross\",\n",
        "    \"hummingbird\", \"woodpecker\", \"bluejay\", \"cardinal\", \"mockingbird\",\n",
        "    \"crow\", \"jay\", \"raven\", \"pigeon\", \"dove\",\n",
        "    \"wren\", \"kingfisher\", \"thrush\", \"warbler\", \"oriole\",\n",
        "    \"chickadee\", \"titmouse\", \"nuthatch\", \"grosbeak\", \"vulture\",\n",
        "    \"osprey\", \"falcon\", \"kestrel\", \"egret\", \"stork\",\n",
        "    \"swallow\", \"swift\", \"tern\", \"puffin\", \"cormorant\",\n",
        "    \"grebe\", \"loon\", \"sandpiper\", \"guillemot\",\n",
        "    \"acorn\", \"apple\", \"aspen\", \"banana\", \"beetle\",\n",
        "    \"berry\", \"blossom\", \"breeze\", \"brook\", \"bush\",\n",
        "    \"buttercup\", \"caterpillar\", \"cherry\", \"clover\", \"cricket\",\n",
        "    \"daisy\", \"dragonfly\", \"dandelion\", \"fog\", \"fawn\",\n",
        "    \"fox\", \"garden\", \"glacier\", \"grape\", \"hedgehog\",\n",
        "    \"honey\", \"ivy\", \"jasmine\", \"moon\", \"juniper\", \"kiwi\",\n",
        "    \"lily\", \"lizard\", \"mango\", \"mushroom\", \"nectar\",\n",
        "    \"nut\", \"peach\", \"pear\", \"petal\", \"poppy\",\n",
        "    \"quail\", \"quince\", \"rabbit\", \"rose\", \"seashell\",\n",
        "    \"snail\", \"spider\", \"thistle\", \"tulip\", \"violet\",\n",
        "    \"willow\", \"zebra\", \"watermelon\", \"zinnia\"\n",
        "]\n",
        "\n",
        "\n",
        "# call\n",
        "filtered_sentence = filter_words(unique_sentence, nature_dataset)\n",
        "print(\"After deep filtering: \"filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2igPO-XaN8F",
        "outputId": "860a96ed-e2ed-4020-e5d2-9da25b1011ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "star sky\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQ1brsCfbMej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}